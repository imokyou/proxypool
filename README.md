# 代理池

## 模块设计
* 调度器schedule，负责调用代理抓取器、代理检测器补充代理数量
* 代理抓取器spider，负责从多个站点抓取代理
* 代理检测器schedule.ProxyTester，负责检测代理是否合法，不合法则从代理池中移除
* 数据存储器db，负责保存代理池数据，主是要用redis的list来实现
* 数据下载器downloader，负责下载网页内容


## 程序设计
* run.py 调用 schedule.run方法，开启两个进程，一个处理代理抓取逻辑，一个处理代理检测逻辑
* 代理抓取逻辑
    * while True 循环，间隔xx秒，查询代理池是否达到阀值
    * 代理池少于设定的阀值下限，则启动spider，循环多个站点爬取器获取代理IP
    * spider利用了python的metaclass，自动注册了多个站点的爬取器
    * 站点爬取器里用了yield实现一个IP生成器，这样的好处是需要用到的时候才抓取IP，不用一次性全拿。否则一个站点几十上百页，一次拿上千个IP，太费资源了(存数据需要内存，解析HTML需要CPU)
    * 把拿到的代理用代理检测器检测schedule.ProxyTester是否有效，有效则调用db存入数据库，无效则存到invalid_proxy缓存，下次再拿到这个代理的时候就不用再次浪费时间检测了
    这里用到了gevent的异步处理机制，如果用python3的话可以用asyncio.
    * 再次检查代理池，如果代理数量达超过阀值上限，就暂停，否则回到开头继续
    * 注意不要一次一起发起过多的请求，每个站点爬取器爬取间隔设置XX秒
* 代理检测逻辑
    * while True 循环， 间隔xx秒
    * 从代理池里拿出XX个代理，放到代理检测器检测schedule.ProxyTester是否有效，有效则调用db存入数据库，无效则存到invalid_proxy缓存，下次再拿到这个代理的时候就不用再次浪费时间检测了
    * 这里用到了redis的lrange、ltrim方法，保证从redis拿出来的代理不在原来的list里的，检测有效再存回去，这样就不会费劲去删无效代理了
* 数据格式
    * redis的list
    * 最新检测过的有效代理用lpush方法，这样可以保证list左边永远是最新有效的代理，这在做API接口的时候很方便(不用再次排序)
* 关于下载器
    * 如果被封IP了，可以利用已拿到的代理发起网络请求，这样循环往复就不怕被封IP了
    * 收集多个headers, 每次用随机一个header发起网络请求
